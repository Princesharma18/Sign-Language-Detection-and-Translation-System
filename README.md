# ğŸ¤Ÿ Sign Language Detection and Translation System

A deep learningâ€“powered system that detects hand gestures from sign language and translates them into text and speech in real time.  
This project aims to improve accessibility by bridging the communication gap between sign language users and non-signers.

---

## ğŸš€ Features
- Real-time **hand gesture detection** using computer vision (OpenCV + MediaPipe).
- **Deep learning model** trained to classify sign language alphabets/words.
- Translates recognized gestures into **text**.
- User-friendly Interface.

---

## ğŸ› ï¸ Tech Stack
- **Python**,**Django**
- **PyTorch** â€“ Model training & prediction
- **OpenCV** â€“ Image processing & webcam capture
- **MediaPipe** â€“ Hand landmark detection
- **NumPy, Pandas, Matplotlib** â€“ Data preprocessing & visualization

---

## How It Works
- Capture live video feed using webcam.
- Detect hand landmarks using MediaPipe.
- Extract features and feed them into the trained model.
- Model predicts the alphabet/word/digit. 
- Display text output.

---
## Preview

<img width="3838" height="1848" alt="Screenshot 2025-08-29 173825" src="https://github.com/user-attachments/assets/e695ee7b-c4a9-4866-87ba-bbaba62778d9" />

